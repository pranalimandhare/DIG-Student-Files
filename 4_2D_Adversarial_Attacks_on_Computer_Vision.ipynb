{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.8",
      "language": "python",
      "name": "python3.8"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranalimandhare/DIG-Student-Files/blob/main/4_2D_Adversarial_Attacks_on_Computer_Vision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGCdmDAKpLuf"
      },
      "source": [
        "Modified from:\n",
        "\n",
        "[https://www.tensorflow.org/beta/tutorials/generative/adversarial_fgsm](https://www.tensorflow.org/beta/tutorials/generative/adversarial_fgsm).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhdzAfDalt6_"
      },
      "source": [
        "GitHub: https://github.com/MYUSER/MYPROJECT/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tViStp65lt7A"
      },
      "source": [
        "Welcome to your assignment this week!\n",
        "\n",
        "To better understand adverse attacks againsts AI and how it is possible to fool an AI system, in this assignment, we will look at a Computer Vision use case.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dn1-g8BpPDx"
      },
      "source": [
        "This assessment creates an *adversarial example* using the Fast Gradient Signed Method (FGSM) attack as described in [Explaining and Harnessing Adversarial Examples](https://arxiv.org/abs/1412.6572) by Goodfellow *et al*. This was one of the first and most popular attacks to fool a neural network.\n",
        "\n",
        "# What is an adversarial example?\n",
        "\n",
        "Adversarial examples are specialised inputs created with the purpose of confusing a neural network, resulting in the misclassification of a given input. These notorious inputs are indistinguishable to the human eye, but cause the network to fail to identify the contents of the image. There are several types of such attacks, however, here the focus is on the fast gradient sign method attack, which is a *white box* attack whose goal is to ensure misclassification. A white box attack is where the attacker has complete access to the model being attacked.\n",
        "\n",
        "# Fast gradient sign method\n",
        "The fast gradient sign method works by using the gradients of the neural network to create an adversarial example. For an input image, the method uses the gradients of the loss with respect to the input image to create a new image that maximises the loss. This new image is called the adversarial image. This can be summarised using the following expression:\n",
        "$$adv\\_x = x + \\epsilon*\\text{sign}(\\nabla_xJ(\\theta, x, y))$$\n",
        "\n",
        "where\n",
        "\n",
        "*   adv_x : Adversarial image.\n",
        "*   x : Original input image.\n",
        "*   y : Original input label.\n",
        "*   $\\epsilon$ : Multiplier to ensure the perturbations are small.\n",
        "*   $\\theta$ : Model parameters.\n",
        "*   $J$ : Loss.\n",
        "\n",
        "An intriguing property here, is the fact that the gradients are taken with respect to the input image. This is done because the objective is to create an image that maximises the loss. A method to accomplish this is to find how much each pixel in the image contributes to the loss value, and add a perturbation accordingly. This works pretty fast because it is easy find how each input pixel contributes to the loss, by using the chain rule, and finding the required gradients. Hence, the gradients are used with respect to the image. In addition, since the model is no longer being trained (thus the gradient is not taken with respect to the trainable variables, i.e., the model parameters), and so the model parameters remain constant. The only goal is to fool an already trained model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veyLjuLxlt7C"
      },
      "source": [
        "# Part 1\n",
        "\n",
        "So let's try and fool a pretrained model. In this first part, the model is [MobileNetV2](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/applications/MobileNetV2) model, pretrained on [ImageNet](http://www.image-net.org/).\n",
        "\n",
        "\n",
        "Run the following cell to install all the packages you will need."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "2odxHU4-lt7D"
      },
      "source": [
        "# ! pip3 install cython\n",
        "# ! pip3 install tensornets\n",
        "# ! pip3 install numpy==1.16.1\n",
        "# ! pip3 install tensorflow\n",
        "# ! pip3 install matplotlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43XMFnApdIzM"
      },
      "source": [
        "#pip install tensornets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzL8AtBMlt7E"
      },
      "source": [
        "Run the following cell to load the packages you will need."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAn9n8hAS68X"
      },
      "source": [
        "pip install tensornets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vag2WYR6yTOC"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import tensornets as nets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWi9Ye0Zlt7H"
      },
      "source": [
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "config.log_device_placement = True\n",
        "config.allow_soft_placement = True\n",
        "sess = tf.Session(config=config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiTHY8dqxzx7"
      },
      "source": [
        "Let's define the computation graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqhk2vYx6Ag0"
      },
      "source": [
        "# Helper function to preprocess the image so that it can be inputted in MobileNetV2\n",
        "def preprocess(image):\n",
        "    image = tf.cast(image, tf.int64)\n",
        "    image = tf.image.resize(image, (224, 224))\n",
        "    image = image /  127.5\n",
        "    image = image - 1.0\n",
        "    image = image[None, ...]\n",
        "    return image\n",
        "def reverse_preprocess(image):\n",
        "    image = image + 1.0\n",
        "    image = image / 2.0\n",
        "    return image\n",
        "\n",
        "# Helper function to extract labels from probability vector\n",
        "def get_imagenet_label(probs):\n",
        "    return decode_predictions(probs, top=5)[0]\n",
        "\n",
        "\n",
        "# Lets's import an image to process.\n",
        "image_path = tf.keras.utils.get_file('YellowLabradorLooking_new.jpg', 'https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg')\n",
        "image_raw = tf.io.read_file(image_path)\n",
        "image = tf.image.decode_png(image_raw)\n",
        "input_image = preprocess(image)\n",
        "reversed_image = reverse_preprocess(input_image)\n",
        "\n",
        "input_image_placeholder = tf.placeholder(shape=[1, 224, 224, 3], dtype=tf.float32)\n",
        "\n",
        "\n",
        "pretrained_model = nets.MobileNet50v2(input_image_placeholder, reuse=tf.AUTO_REUSE)\n",
        "\n",
        "\n",
        "# node to load pretrained weights\n",
        "pretrained_ops = pretrained_model.pretrained()\n",
        "\n",
        "# decode predicted probabilities to ImageNet labels\n",
        "decode_predictions = tf.keras.applications.mobilenet_v2.decode_predictions\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEZaMVFgSUA-"
      },
      "source": [
        "## Original image\n",
        "Let's use a sample image of a [Labrador Retriever](https://commons.wikimedia.org/wiki/File:YellowLabradorLooking_new.jpg) -by Mirko       [CC-BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/) from Wikimedia Common and create adversarial examples from it. The first step is to preprocess it so that it can be fed as an input to the MobileNetV2 model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpYrQ4OQSYWk"
      },
      "source": [
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "config.log_device_placement = True\n",
        "sess = tf.Session(config=config)\n",
        "\n",
        "sess.run(pretrained_ops)\n",
        "preprocessed_img, reversed_img = sess.run([input_image, reversed_image])\n",
        "image_probs = sess.run([pretrained_model], {input_image_placeholder:preprocessed_img})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvPlta_uSbuI"
      },
      "source": [
        "Let's have a look at the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99Jc-SNoSZot",
        "scrolled": false
      },
      "source": [
        "top5 = get_imagenet_label(image_probs[0])\n",
        "tick_names = [x[1] for x in top5]\n",
        "print(tick_names)\n",
        "probs = [x[2] for x in top5]\n",
        "plt.figure(figsize=(9, 3))\n",
        "plt.subplot(121)\n",
        "plt.imshow(reversed_img[0])\n",
        "plt.title('image')\n",
        "ax = plt.gca()\n",
        "ax.axis('off')\n",
        "\n",
        "plt.subplot(122)\n",
        "tick_names = [x[1] for x in reversed(top5)]\n",
        "probs = [x[2] for x in reversed(top5)]\n",
        "plt.barh(tick_names, probs)\n",
        "plt.yticks(rotation=25)\n",
        "ax = plt.gca()\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.spines['left'].set_visible(False)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kElVTbF690CF"
      },
      "source": [
        "# Create the adversarial image\n",
        "\n",
        "## Implementing fast gradient sign method\n",
        "The first step is to create perturbations which will be used to distort the original image resulting in an adversarial image. As mentioned, for this task, the gradients are taken with respect to the image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmLBE-ZClt7P"
      },
      "source": [
        "**TASK 1:** Implement `create_adversarial_pattern()`. You will need to carry out 3 steps:\n",
        "\n",
        "1. Create a loss object using `loss_object` using two arguments: `pretrained_model` and `input_label`.\n",
        "2. Get the gradients using `tf.gradients` of the `loss`  w.r.t to the `input_image`.\n",
        "3. Get the sign of the gradients to create the perturbation using `tf.sign`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Now9GkSx8OcM"
      },
      "source": [
        "input_label.dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhZxlOnuBCVr"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "#tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "#pretrained_model = tf.cast(pretrained_model, tf.int64)\n",
        "def create_adversarial_pattern(input_image, input_label):\n",
        "    ## START YOU CODE HERE (3 lines)\n",
        "\n",
        "    loss =  loss_object( input_label, pretrained_model(input_image))\n",
        "    gradient = tf.gradient(loss, input_image)\n",
        "\n",
        "    signed_grad = tf.sign(gradient)\n",
        "    signed_grad = tf.cast(signed_grad, tf.int64)\n",
        "\n",
        "    # END\n",
        "    return signed_grad\n",
        "create_adversarial_pattern(input_image_placeholder, tf.argmax(pretrained_model,1 ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Hcy5c_k6LDK"
      },
      "source": [
        "input_image_placeholder.dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAq9Nowv6c_W"
      },
      "source": [
        "tf.argmax(pretrained_model,1 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbuftX0eSlDQ"
      },
      "source": [
        "The resulting perturbations can also be visualised."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVjnb6M7Smv4",
        "scrolled": false
      },
      "source": [
        "perturbations = create_adversarial_pattern(input_image_placeholder, tf.argmax(pretrained_model,1 ))\n",
        "p_cliped = tf.clip_by_value(perturbations, 0, 1)\n",
        "\n",
        "p_cliped_val = sess.run(p_cliped, {input_image_placeholder: preprocessed_img})\n",
        "plt.figure()\n",
        "plt.imshow(p_cliped_val[0])\n",
        "plt.gca().axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8ZM4D8Alt7Q"
      },
      "source": [
        "## Fool the AI system"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKKSFHjwCyQH"
      },
      "source": [
        "Let's try this out for different values of epsilon and observe the resultant image. You'll notice that as the value of epsilon is increased, it becomes easier to fool the network, however, this comes as a trade-off which results in the perturbations becoming more identifiable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBtG0Kl5SspV"
      },
      "source": [
        "def display_images(image, description):\n",
        "    rev_image = reverse_preprocess(image)\n",
        "    adv_img, raw_adv_img = sess.run([image, rev_image], {input_image_placeholder: preprocessed_img})\n",
        "    img_probs = sess.run(pretrained_model, {input_image_placeholder: adv_img})\n",
        "    top5 = get_imagenet_label(img_probs)\n",
        "    top5 = list(reversed(top5))\n",
        "    plt.figure(figsize=(9, 3))\n",
        "    plt.subplot(121)\n",
        "    plt.imshow(raw_adv_img[0])\n",
        "    plt.title(description)\n",
        "    plt.gca().axis('off')\n",
        "    plt.subplot(122)\n",
        "    tick_names = [x[1] for x in top5]\n",
        "    probs = [x[2] for x in top5]\n",
        "    plt.barh(tick_names, probs)\n",
        "    plt.yticks(rotation=25)\n",
        "    ax = plt.gca()\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "    ax.spines['left'].set_visible(False)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEID-mCBlt7R"
      },
      "source": [
        "**TASK 2:** Generate adverse image using different values for $\\epsilon$:\n",
        "\n",
        "- adv_x = input_image + $\\epsilon$ * perturbations\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DA8g-Zp69J4",
        "scrolled": false
      },
      "source": [
        "epsilons = [0, 0.01, 0.1, 0.15, 0.3]\n",
        "descriptions = [('Epsilon = {:0.3f}'.format(eps) if eps else 'Input')\n",
        "                for eps in epsilons]\n",
        "\n",
        "for i, eps in enumerate(epsilons):\n",
        "    ## START YOU CODE HERE\n",
        "    adv_x = image +  eps  * perturbations\n",
        "    adv_x = tf.clip_by_value(adv_x, -1, 1)\n",
        "    ## End\n",
        "    adv_x = tf.clip_by_value(adv_x, -1, 1)\n",
        "    display_images(adv_x, descriptions[i])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vm5qN8pXlt7R"
      },
      "source": [
        "**TASK 3: What do you abserve?**\n",
        "\n",
        "***\n",
        "\n",
        "PLEASE ANSWER HERE!\n",
        "\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydcKKiZelt7R"
      },
      "source": [
        "#  Part 2\n",
        "\n",
        "Here, you are required to process adversarial attacks using FGSM for a small subset of [ImageNet Dataset](http://www.image-net.org/). We prepared 100 images from different categories (in `./input_dir/`), and the labels are encoded in `./input_dir/clean_image.list`.\n",
        "\n",
        "For evaluation, each adversarial image generated by the attack model will be fed to an evaluation model, and we will calculate the successful rate of adversarial attacks. **The adversarial images that can fool the evaluation model with $\\epsilon$ = 0.01 will be considered as a success**.\n",
        "\n",
        "\n",
        "\n",
        "**Task 4: Goal**\n",
        "\n",
        "***\n",
        "\n",
        "With the previous FGSM example, you are required to implement an FGSM attack against all examples and calculate the success rate. Also, display the original image with the attacked image as well as the predicted class for each image.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WhuSijylt7S"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0q2yNfylt7S"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4Vnm__2lt7S"
      },
      "source": [
        "# Part 3\n",
        "\n",
        "Open question: Can you implement another computer vision adversarial method? You can try the \"Targeted Fast Gradient Sign Method (T-FGSM)\". Please do so an document your code.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLg2IR64lt7S"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1pvqQnIlt7S"
      },
      "source": [
        "# Congratulations!\n",
        "\n",
        "You've come to the end of this assignment, and have seen a lot of the ways attack and fool an AI system. Here are the main points you should remember:\n",
        "\n",
        "- It is very easy to fool a computer vision system if you know the model and its parameters.\n",
        "- When designing an AI system, you need to think of adverse attacks againsts your system.\n",
        "\n",
        "Congratulations on finishing this notebook!\n",
        "\n"
      ]
    }
  ]
}